{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.dataiku.com/how-to-perform-basic-ml-serving-with-python-docker-kubernetes\n",
    "! curl -X POST localhost:8080/predict -d '{\"data\": [5.1, 3.5, 1.4, 0.2]}' -H 'Content-Type: application/json'\n",
    "{\n",
    "\"prediction\": \"0\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.From ChatGPT: How to build Flask API for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from flask import Flask, request, jsonify\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    # get data from request\n",
    "    data = request.get_json(force=True)\n",
    "    # convert data to dataframe\n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    # load model\n",
    "    model = joblib.load('./models/titanic_model.pkl')\n",
    "    # make prediction\n",
    "    prediction = model.predict(df)\n",
    "    # return prediction\n",
    "    return jsonify({'prediction': prediction.tolist()})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=5000, debug=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this script, you will need to have a trained machine learning model saved as a pickle file in the ./models folder. You can then start the Flask API by running python api.py.\n",
    "\n",
    "You can test the API by making a POST request to the /predict endpoint with a JSON payload containing the features for which you want to make a prediction. For example, using curl, you can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST -H \"Content-Type: application/json\" -d '{\n",
    "    \"Pclass\": 3,\n",
    "    \"Sex\": \"male\",\n",
    "    \"Age\": 22,\n",
    "    \"SibSp\": 1,\n",
    "    \"Parch\": 0,\n",
    "    \"Fare\": 7.25,\n",
    "    \"Embarked\": \"S\"\n",
    "    }' http://localhost:5000/predict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should return a JSON response with the prediction for the input data.\n",
    "e.g. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "\"prediction\": \"1\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.For batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    # get data from request\n",
    "    data = request.get_json(force=True)\n",
    "    # convert data to list of dictionaries\n",
    "    samples = [sample for sample in data]\n",
    "    # convert to dataframe\n",
    "    df = pd.DataFrame.from_dict(samples)\n",
    "    # load model\n",
    "    model = joblib.load('./models/titanic_model.pkl')\n",
    "    # make prediction\n",
    "    prediction = model.predict(df)\n",
    "    # return prediction\n",
    "    return jsonify({'prediction': prediction.tolist()})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then make a batch prediction by sending a list of samples in the request payload, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST -H \"Content-Type: application/json\" -d '[\n",
    "  {\n",
    "    \"Pclass\": 3,\n",
    "    \"Sex\": \"male\",\n",
    "    \"Age\": 22,\n",
    "    \"SibSp\": 1,\n",
    "    \"Parch\": 0,\n",
    "    \"Fare\": 7.25,\n",
    "    \"Embarked\": \"S\"\n",
    "  },\n",
    "  {\n",
    "    \"Pclass\": 3,\n",
    "    \"Sex\": \"female\",\n",
    "    \"Age\": 32,\n",
    "    \"SibSp\": 0,\n",
    "    \"Parch\": 0,\n",
    "    \"Fare\": 7.9250,\n",
    "    \"Embarked\": \"S\"\n",
    "  }\n",
    "]' http://localhost:5000/predict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should return a list of predictions for each sample in the input list.\n",
    "\n",
    "If you make a batch prediction with the modified predict function that I provided, the response will be a JSON object with a single key, prediction, that maps to a list of predictions, where each prediction is a string representing the predicted class (either \"0\" or \"1\" in this case, since it's a binary classification problem).\n",
    "\n",
    "Here is an example of the response you might receive for the batch prediction request that I provided in the previous message:\n",
    "\n",
    "`{\n",
    "  \"prediction\": [\"0\", \"1\"]\n",
    "}`\n",
    "\n",
    "\n",
    "Each element in the prediction list corresponds to the prediction for one of the samples in the input list. In this example, the first sample is predicted to belong to class \"0\", and the second sample is predicted to belong to class \"1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"prediction\": [\"0\", \"1\"]\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.To deploy the Flask API in a Docker container, \n",
    "\n",
    "you will need to do the following:\n",
    "\n",
    "1. Write a Dockerfile that specifies the steps to build the Docker image. The Dockerfile should start with a base image, such as python:3, and then use pip to install the required Python libraries. It should also copy the API script and any other necessary files to the image.\n",
    "\n",
    "\n",
    "Here is an example Dockerfile that shows how to build the API image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dockerfile\n",
    "FROM python:3\n",
    "\n",
    "# Install required libraries\n",
    "RUN pip install pandas scikit-learn flask\n",
    "\n",
    "# Copy API script\n",
    "COPY api.py .\n",
    "\n",
    "# Copy model file\n",
    "COPY models/titanic_model.pkl models/\n",
    "\n",
    "# Set the entrypoint to the API script\n",
    "ENTRYPOINT [\"python\", \"api.py\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Build the Docker image using the Dockerfile. You can do this by running the following command in the same directory as the Dockerfile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -t titanic-api ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a Docker image with the name titanic-api that contains the API and all the necessary dependencies.\n",
    "\n",
    "3. Run the Docker image as a container using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -p 5000:5000 titanic-api"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will start a container based on the titanic-api image and expose the API on port 5000 of the container. The -p 5000:5000 flag maps port 5000 of the container to port 5000 on the host machine, so you can access the API at http://localhost:5000.\n",
    "\n",
    "That's it! You should now be able to access the API using the same requests as before, but now it will be running inside a Docker container."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: So after that we shall be connected to the bash interpreter of the docker container, and then we can still run exactly the same command to perform inference ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST -H \"Content-Type: application/json\" -d '[\n",
    "  {\n",
    "    \"Pclass\": 3,\n",
    "    \"Sex\": \"male\",\n",
    "    \"Age\": 22,\n",
    "    \"SibSp\": 1,\n",
    "    \"Parch\": 0,\n",
    "    \"Fare\": 7.25,\n",
    "    \"Embarked\": \"S\"\n",
    "  },\n",
    "  {\n",
    "    \"Pclass\": 3,\n",
    "    \"Sex\": \"female\",\n",
    "    \"Age\": 32,\n",
    "    \"SibSp\": 0,\n",
    "    \"Parch\": 0,\n",
    "    \"Fare\": 7.9250,\n",
    "    \"Embarked\": \"S\"\n",
    "  }\n",
    "]' http://localhost:5000/predict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: No, you don't need to be connected to the bash interpreter of the Docker container to make predictions with the API. Once you have started the container by running docker run -p 5000:5000 titanic-api, the API will be running in the background and you can make requests to it from your host machine.\n",
    "\n",
    "To make a prediction, you can use the same curl command that I provided earlier, replacing http://localhost:5000 with the address of the host machine where the Docker container is running. If you are running the container on your local machine and the host's IP address is 127.0.0.1, you can use the following command to make a prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST -H \"Content-Type: application/json\" -d '[\n",
    "  {\n",
    "    \"Pclass\": 3,\n",
    "    \"Sex\": \"male\",\n",
    "    \"Age\": 22,\n",
    "    \"SibSp\": 1,\n",
    "    \"Parch\": 0,\n",
    "    \"Fare\": 7.25,\n",
    "    \"Embarked\": \"S\"\n",
    "  },\n",
    "  {\n",
    "    \"Pclass\": 3,\n",
    "    \"Sex\": \"female\",\n",
    "    \"Age\": 32,\n",
    "    \"SibSp\": 0,\n",
    "    \"Parch\": 0,\n",
    "    \"Fare\": 7.9250,\n",
    "    \"Embarked\": \"S\"\n",
    "  }\n",
    "]' http://127.0.0.1:5000/predict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should return a JSON object with the predictions for each sample in the input list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"prediction\": [\"0\", \"1\"]\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.To store the JSON responses to the API request, \n",
    "\n",
    "you can use the -o option of curl to specify an output file where the response will be saved. For example, to save the response to a file named response.json, you can use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -X POST -H \"Content-Type: application/json\" -d '{\n",
    "  \"Pclass\": 3,\n",
    "  \"Sex\": \"male\",\n",
    "  \"Age\": 22,\n",
    "  \"SibSp\": 1,\n",
    "  \"Parch\": 0,\n",
    "  \"Fare\": 7.25,\n",
    "  \"Embarked\": \"S\"\n",
    "}' http://localhost:5000/predict -o response.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will send a POST request to the API and save the response to the response.json file. The file will contain the raw JSON response, including the headers and other metadata.\n",
    "\n",
    "If you want to save only the JSON payload of the response, you can use the jq command-line JSON processor to extract the payload and save it to a file. For example, to save the payload to a file named response_payload.json, you can use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -X POST -H \"Content-Type: application/json\" -d '{\n",
    "  \"Pclass\": 3,\n",
    "  \"Sex\": \"male\",\n",
    "  \"Age\": 22,\n",
    "  \"SibSp\": 1,\n",
    "  \"Parch\": 0,\n",
    "  \"Fare\": 7.25,\n",
    "  \"Embarked\": \"S\"\n",
    "}' http://localhost:5000/predict | jq '.prediction' > response_payload.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will send the same request to the API and save the prediction field of the response to the response_payload.json file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.My own notes on data type manipulations for handling API input data and feeding into the ML model as dataframe\n",
    "\n",
    "See more tricks at https://towardsdatascience.com/how-to-convert-json-into-a-pandas-dataframe-100b2ae1e0d8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5.1, 3.5, 1.4, 0.2]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\"data\": [5.1, 3.5, 1.4, 0.2]}\n",
    "\n",
    "input_data = [data[\"data\"]]\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col 1</th>\n",
       "      <th>col 2</th>\n",
       "      <th>col 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>row 1</th>\n",
       "      <td>51</td>\n",
       "      <td>61</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row 2</th>\n",
       "      <td>121</td>\n",
       "      <td>118</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       col 1  col 2  col 3\n",
       "row 1     51     61     91\n",
       "row 2    121    118    127"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "# Method to convert 2D-arrays to JSON via Pandas to serialize autmatically. \n",
    "# circumvents error: 'Object of type ndarray is not JSON serializable'\n",
    "\n",
    "numpyArray = numpy.array([[51, 61, 91], [121, 118, 127]])\n",
    "\n",
    "df = pd.DataFrame(numpyArray, index=['row 1', 'row 2'], columns=['col 1', 'col 2', 'col 3'])\n",
    "df_json = df.to_json(orient='index')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"row 1\":{\"col 1\":51,\"col 2\":61,\"col 3\":91},\"row 2\":{\"col 1\":121,\"col 2\":118,\"col 3\":127}}'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col 1</th>\n",
       "      <th>col 2</th>\n",
       "      <th>col 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51</td>\n",
       "      <td>61</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121</td>\n",
       "      <td>118</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col 1  col 2  col 3\n",
       "0     51     61     91\n",
       "1    121    118    127"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame(numpyArray,  columns=['col 1', 'col 2', 'col 3'])\n",
    "df_json2 = df2.to_json(orient='index')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"0\":{\"col 1\":51,\"col 2\":61,\"col 3\":91},\"1\":{\"col 1\":121,\"col 2\":118,\"col 3\":127}}'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json2_array_of_dicts = [df_json2.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_json(df_json, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col 1</th>\n",
       "      <th>col 2</th>\n",
       "      <th>col 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>row 1</th>\n",
       "      <td>51</td>\n",
       "      <td>61</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row 2</th>\n",
       "      <td>121</td>\n",
       "      <td>118</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       col 1  col 2  col 3\n",
       "row 1     51     61     91\n",
       "row 2    121    118    127"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['col 1', 'col 2', 'col 3']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols = list(dataframe.columns)\n",
    "feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'col 1': {'row 1': 51, 'row 2': 121},\n",
       " 'col 2': {'row 1': 61, 'row 2': 118},\n",
       " 'col 3': {'row 1': 91, 'row 2': 127}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'col 1': 51, 'col 2': 61, 'col 3': 91},\n",
       " {'col 1': 121, 'col 2': 118, 'col 3': 127}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict = dataframe.to_dict(\"records\") # Note this output format is the JSON format (list of dicts with col names as keys) given in the input API\n",
    "df_dict "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert to dataframe for conversion we can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x3 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "dictvectorizer = DictVectorizer(sort=False)\n",
    "csr_matrix = dictvectorizer.fit_transform(df_dict)\n",
    "csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col 1</th>\n",
       "      <th>col 2</th>\n",
       "      <th>col 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>127.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col 1  col 2  col 3\n",
       "0   51.0   61.0   91.0\n",
       "1  121.0  118.0  127.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict_revert = pd.DataFrame(csr_matrix.toarray(),columns=feature_cols)\n",
    "df_dict_revert"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, you can deploy a Flask API on Amazon Web Services (AWS). There are several ways to do this, depending on your specific needs and requirements. Here are some options you can consider:\n",
    "\n",
    "AWS Elastic Beanstalk: This is a fully managed service that makes it easy to deploy and run web applications and services. You can deploy your Flask API to Elastic Beanstalk by packaging it as a Docker container and uploading it to the service. Elastic Beanstalk will then handle the details of deploying and scaling your application.\n",
    "\n",
    "AWS Lambda: This is a serverless computing platform that allows you to run code in response to events or automatically scale. You can use AWS Lambda to host your Flask API by wrapping it in a function and deploying it to the service. You can then trigger the function using an API Gateway, which provides a fully managed HTTP service for creating, publishing, and maintaining APIs.\n",
    "\n",
    "AWS EC2: This is a web service that provides resizable compute capacity in the cloud. You can use an EC2 instance to host your Flask API by installing the required dependencies, including Python and Flask, on the instance and running the API script.\n",
    "\n",
    "I hope this gives you an idea of the options available for deploying a Flask API on AWS. Let me know if you have any questions or need further assistance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Pls tell me more about how to deploy Flask API in AWS EC2?\n",
    "To deploy a Flask API on an Amazon EC2 instance, you will need to do the following:\n",
    "\n",
    "Launch an EC2 instance: First, you will need to launch an EC2 instance using the Amazon EC2 management console, the AWS CLI, or an AWS SDK. Choose an Amazon Machine Image (AMI) that includes the required dependencies, such as Python and Flask, or create your own AMI with the necessary dependencies installed.\n",
    "\n",
    "Connect to the EC2 instance: Once the instance is launched, you will need to connect to it using Secure Shell (SSH). You will need the instance's public DNS name or IP address, as well as the private key file that you specified when you launched the instance.\n",
    "\n",
    "Copy the API files to the EC2 instance: Once you are connected to the EC2 instance, you will need to copy the API script and any other necessary files to the instance. You can use a tool such as scp to copy the files over SSH.\n",
    "\n",
    "Install dependencies: If you are using an AMI that does not include the necessary dependencies, you will need to install them using a package manager such as pip or apt.\n",
    "\n",
    "Run the API: Finally, you can start the API by running the API script on the EC2 instance. You can use a tool such as nohup or screen to run the script in the background, so that it continues to run even if you close the SSH connection.\n",
    "\n",
    "That's it! Your Flask API should now be running on the EC2 instance and be accessible from the Internet.\n",
    "\n",
    "I hope this helps! Let me know if you have any questions or need further assistance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "So the EC2 instance is a VM?\n",
    "Yes, an Amazon Elastic Compute Cloud (EC2) instance is a virtual machine (VM) provided by the Amazon Web Services (AWS) cloud computing platform. When you launch an EC2 instance, you can choose from a variety of pre-configured templates called Amazon Machine Images (AMIs) that include different operating systems, software packages, and configurations. You can also create your own custom AMI with the specific dependencies that you need.\n",
    "\n",
    "Once you have launched an EC2 instance, you can connect to it using Secure Shell (SSH) and treat it like a regular server or desktop machine. You can install software, run applications, and store data on the instance, just as you would on a physical machine.\n",
    "\n",
    "EC2 instances are scalable and flexible, allowing you to choose the right combination of resources (such as CPU, memory, and storage) for your workload. You can also choose the type of instance that best fits your needs, such as a general-purpose instance, a compute-optimized instance, or a GPU-powered instance.\n",
    "\n",
    "I hope this helps! Let me know if you have any questions or need further assistance.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's difficult to say which option is the \"best\" for deploying a Flask API, as it depends on your specific needs and requirements. Here are some factors to consider when choosing which option to use:\n",
    "\n",
    "AWS Elastic Beanstalk: This is a fully managed service that makes it easy to deploy and run web applications and services. It is a good choice if you want a simple and hassle-free way to deploy your API, as it handles the details of deploying and scaling your application automatically. However, it may not offer as much control or customization as other options.\n",
    "\n",
    "AWS Lambda: This is a serverless computing platform that allows you to run code in response to events or automatically scale. It is a good choice if you want to pay only for the resources that you consume and do not want to worry about managing servers. However, it may not be suitable for applications that require long-running processes or need to store large amounts of data.\n",
    "\n",
    "AWS EC2: This is a web service that provides resizable compute capacity in the cloud. It is a good choice if you need full control over the underlying infrastructure and want to customize the operating system, network, and other settings of the instance. However, it requires more maintenance and management than fully managed services such as Elastic Beanstalk."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Here are the detailed steps for deploying a Flask API to Amazon Web Services (AWS) Elastic Beanstalk:\n",
    "\n",
    "Package the API as a Docker container: AWS Elastic Beanstalk supports deploying Docker containers, so you will need to package your Flask API as a Docker container. To do this, you will need to create a Dockerfile that defines the dependencies and steps needed to build the container image.\n",
    "\n",
    "Build the Docker image: Once you have created the Dockerfile, you can build the Docker image using the docker build command. This will create a local copy of the image that you can use to test and debug the API.\n",
    "\n",
    "Test the Docker image: Before deploying the Docker image to Elastic Beanstalk, you should test it to make sure that it is working as expected. You can use the docker run command to start a container from the image and test the API locally.\n",
    "\n",
    "Push the Docker image to Amazon Elastic Container Registry (ECR): AWS Elastic Beanstalk requires that the Docker image be stored in an Amazon Elastic Container Registry (ECR) repository. To push the image to ECR, you will need to create a repository and then use the docker push command to upload the image.\n",
    "\n",
    "Create an Elastic Beanstalk application: Next, you will need to create an Elastic Beanstalk application using the AWS Management Console or the AWS CLI. This will create the necessary resources and permissions for deploying the API.\n",
    "\n",
    "Create an Elastic Beanstalk environment: After creating the application, you will need to create an Elastic Beanstalk environment within the application. An environment is a logical deployment group of Amazon EC2 instances, Amazon RDS DB instances, or both. You can choose the type of environment that best fits your needs, such as a web server environment or a worker environment.\n",
    "\n",
    "Deploy the Docker image to Elastic Beanstalk: Finally, you can deploy the Docker image to Elastic Beanstalk using the AWS Management Console, the AWS CLI, or the Elastic Beanstalk API. Elastic Beanstalk will handle the details of deploying and scaling the API automatically.\n",
    "\n",
    "That's it! Your Flask API should now be running on Elastic Beanstalk and be accessible from the Internet.\n",
    "\n",
    "I hope this helps! Let me know if you have any questions or need further assistance.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the detailed steps for deploying a Flask API to Amazon Web Services (AWS) Lambda:\n",
    "\n",
    "1. Create a Python function: AWS Lambda allows you to run code in response to events or automatically scale. To use Lambda to host your Flask API, you will need to wrap the API in a Python function and define a handler for incoming requests. Here is an example of what the function might look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from flask import Flask, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/predict\", methods=[\"POST\"])\n",
    "def predict():\n",
    "    # Parse the request data\n",
    "    data = request.get_json()\n",
    "\n",
    "    # Perform predictions using the model\n",
    "    predictions = model.predict(data)\n",
    "\n",
    "    # Return the predictions as a JSON response\n",
    "    return json.dumps(predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Package the function and dependencies: Next, you will need to package the function and any dependencies, such as Flask and the API's model, into a deployable package. You can do this using a tool such as pip to create a requirements.txt file and then using zip to create a zip archive of the files.\n",
    "\n",
    "3. Create an IAM role: AWS Lambda requires an IAM role that grants the necessary permissions to execute the function. You can create an IAM role using the AWS Management Console or the AWS CLI.\n",
    "\n",
    "4. Create a Lambda function: After creating the IAM role, you can create a Lambda function using the AWS Management Console, the AWS CLI, or the Lambda API. You will need to provide the deployable package, the IAM role, and the name of the handler function.\n",
    "\n",
    "5. Create an API Gateway: To make the Lambda function accessible over HTTP, you will need to create an Amazon API Gateway. This will create a fully managed HTTP service for creating, publishing, and maintaining APIs.\n",
    "\n",
    "6. Create a stage and deployment: After creating the API Gateway, you will need to create a stage and deployment to define how the API should behave. You can specify settings such as the base path, security, and caching options.\n",
    "\n",
    "7. Test the API: Finally, you can test the API using a tool such as curl or by using the API Gateway's built-in test interface.\n",
    "\n",
    "That's it! Your Flask API should now be running on Lambda and be accessible through the API Gateway."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the detailed steps for deploying a Flask API to Amazon Web Services (AWS) EC2:\n",
    "\n",
    "Launch an EC2 instance: First, you will need to launch an EC2 instance using the Amazon EC2 management console, the AWS CLI, or an AWS SDK. Choose an Amazon Machine Image (AMI) that includes the required dependencies, such as Python and Flask, or create your own AMI with the necessary dependencies installed.\n",
    "\n",
    "Connect to the EC2 instance: Once the instance is launched, you will need to connect to it using Secure Shell (SSH). You will need the instance's public DNS name or IP address, as well as the private key file that you specified when you launched the instance.\n",
    "\n",
    "Install dependencies: If you are using an AMI that does not include the necessary dependencies, you will need to install them using a package manager such as pip or apt.\n",
    "\n",
    "Copy the API files to the EC2 instance: Once you are connected to the EC2 instance, you will need to copy the API script and any other necessary files to the instance. You can use a tool such as scp to copy the files over SSH.\n",
    "\n",
    "Run the API: Finally, you can start the API by running the API script on the EC2 instance. You can use a tool such as nohup or screen to run the script in the background, so that it continues to run even if you close the SSH connection.\n",
    "\n",
    "That's it! Your Flask API should now be running on the EC2 instance and be accessible from the Internet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 13:09:58) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40c94175794199cf7bd0859a333ef20cc625dc77ebef68b8004e895f10df4d5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
